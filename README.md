# Algoritmo genético de pesos
Nesta abordagem será utilizado um algoritmo genético que treinará a camada densa de uma rede. Essa rede é composta, em sua parte convolucional, pela rede InceptionV3 da Google, juntamente com um GlobalAveragePooling2D para reduzir o número de características retornadas. Na parte densa, temos mais três camadas: a primeira contendo 1024 neurônios e ativação Relu, junto com um Dropout de 0.25; a segunda contendo 512 neurônios e ativação Relu, com Dropout de 0.5; e a última camada, que representa a saída da rede, contendo 12 neurônios e ativação Sigmoid. A escolha dessa configuração de saída é definida pela quantidade de botões do controle do Super Nintendo. O motivo de usar Sigmoid para a saída é que, com esse tipo de ativação, é possível ter mais de um retorno com probabilidade de ser 1. Isso é fundamental, pois em jogos, muitas vezes temos que apertar vários botões simultaneamente para realizar ações, como, por exemplo, andar para frente e pular. Dessa forma, quando na saída temos 1, significa que o botão deve ser apertado; caso contrário, o valor é 0, indicando que o botão não deve ser pressionado.

Nesta abordagem, inicialmente criamos os pesos iniciais de cada um dos 25 agentes da primeira população utilizando o método initializers.HeUniform() fornecido pela biblioteca Python do Keras.

Após definidos os pesos de cada agente, realiza-se a próxima etapa desta abordagem, que consiste em avaliar a aptidão de cada agente em um determinado trecho do jogo "Daffy Duck: The Marvin Missions". Cada agente é testado por um total de 4001 iterações, que representam as respostas do agente para o quadro atual do jogo. Durante cada iteração, a função de avaliação de aptidão captura o estado atual do jogo, assim como três informações armazenadas na memória do jogo: a quantidade de vidas (tentativas), o valor da barra de vida atual e a pontuação.

Através desses dados, são realizados cálculos para atribuir uma pontuação ao desempenho do agente. Inicialmente, cada agente começa com uma pontuação de 300000 (trezentos mil) pontos. Caso o agente aumente sua pontuação durante uma iteração, ele ganha 100000 (cem mil) pontos adicionais; caso contrário, perde 100 pontos. Se a barra de vida do agente diminuir, são subtraídos 500 (quinhentos) pontos; se a quantidade de vidas diminuir, a penalidade é de 50000 (cinquenta mil) pontos. Além disso, a cada mil iterações, verifica-se se o agente tomou alguma ação com base nos indicadores de pontuação, barra de vida e quantidade de vidas. Caso não tenha tomado nenhuma ação, o agente perde 300000 (trezentos mil) pontos.

Essa lógica foi desenvolvida com o objetivo de não recompensar o agente por ações que ele só teria condições de realizar caso tivesse tomado ações inadequadas anteriormente. Por exemplo, a ação de aumentar a barra de vida só é possível se o agente já tiver perdido vida anteriormente. Além disso, também é considerado importante penalizar os agentes que permanecem parados ou tomam direções incorretas durante as iterações.

Após avaliados todos os 25 agentes, a função retorna uma lista com a pontuação alcançada por cada um deles. Essa lista é, então, passada para a função de seleção de elites, que ordenará a lista de forma decrescente e selecionará as dez primeiras posições, correspondentes aos dez melhores agentes. Após a seleção dos melhores agentes, é invocada a função que gerará a nova população. Nessa função, é realizado o cruzamento uniforme, no qual os pesos de dois indivíduos (pais) são mesclados de maneira aleatória. Após a etapa de cruzamento, a função responsável pela mutação é acionada. Nesse processo, é inserido um ruído aleatório nos pesos de cada indivíduo, baseando-se na taxa de mutação de 0.05.

Por fim, após a realização do cruzamento e da mutação, é criada uma nova população. Essa população combina os indivíduos da elite original com os novos indivíduos gerados por cruzamento e mutação. A cada execução do algoritmo, as pontuações máxima e média de cada geração são calculadas e armazenadas. Quando o programa é interrompido, os dados são exibidos graficamente para o usuário.

# Configuração do ambiente

Para a realização deste trabalho, foi necessário criar e configurar um ambiente de desenvolvimento local em uma pasta no Windows 11. Utilizou-se o Python na versão 3.8 e as biblioteca Gym na versão 0.21.0 e Gym Retro na versão 0.8.0. É importante destacar que versões mais recentes das bibliotecas mencionadas são incompatíveis com versões anteriores.

Após a instalação do ambiente, é preciso obter uma ROM do jogo que se deseja utilizar para os testes. Essa ROM precisa ter a mesma SHA definida pelo Gym Retro em suas instalações para o jogo em questão. Cabe ressaltar que nem todos os jogos são suportados pelo Gym Retro.

Uma vez que o ambiente e o jogo estão configurados, é possível compilar e executar o código Python. Basta abrir o prompt de comando na pasta onde todas as configurações foram feitas, digitar Scripts$\backslash$activate e pressionar "Enter". Em seguida, o código pode ser executado usando o comando python nome\_do\_arquivo.py.

# Requisitos

A validação deste projeto foi realizada por meio de testes em um computador com 32 GB de memória RAM e uma CPU Intel Core i5-11600k. Dessa forma, foram feitas observações sobre a viabilidade das abordagens definidas, considerando as limitações do hardware, bem como sua performance na execução do jogo com o algoritmo.
